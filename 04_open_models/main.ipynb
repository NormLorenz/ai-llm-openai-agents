{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNLHsrWVI5zLIS196WJ9S0U",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NormLorenz/ai-llm-openai-agents/blob/main/04_open_models/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lesson 4\n",
        "\n",
        "### Take Aways\n",
        "* Open Source Models\n",
        "* Using Ollama and Groq\n",
        "* Set the model at the global instance, agent instance or run instance\n",
        "\n",
        "https://platform.openai.com/logs?api=traces\n",
        "\n",
        "https://www.youtube.com/playlist?list=PLO66QfE8gWT0oM1hbfcFUa-2H3yI4vfg8"
      ],
      "metadata": {
        "id": "tJeLCIh43_F2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "\n",
        "!pip install openai-agents"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Sahq_mNm3bM-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import warnings\n",
        "from google.colab import userdata\n",
        "\n",
        "# Filter specific DeprecationWarning from jupyter_client\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning, module='jupyter_client')\n",
        "\n",
        "# Initialize API key\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\")"
      ],
      "metadata": {
        "id": "dnjmUqxO3rE_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from agents import (\n",
        "    AsyncOpenAI, Agent,\n",
        "    OpenAIChatCompletionsModel,\n",
        "    Runner, set_tracing_disabled,\n",
        "    RunConfig, ModelProvider, Model,\n",
        "    set_default_openai_client,\n",
        "    set_default_openai_api\n",
        "\n",
        ")\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "set_tracing_disabled(True)\n",
        "\n",
        "# ollama client\n",
        "# client = AsyncOpenAI(\n",
        "#     base_url=\"http://localhost:11434/v1\"\n",
        "# )\n",
        "\n",
        "# # Option 1 : Set the model at Agent instance\n",
        "# agent = Agent(\n",
        "#     name = \"python agent\",\n",
        "#     instructions = \"You are a helpful assistant that helps with Python programming.\",\n",
        "#     model = OpenAIChatCompletionsModel(\n",
        "#         model = \"llama3.2\",\n",
        "#         openai_client= client\n",
        "#     )\n",
        "# )\n",
        "\n",
        "# run the agent\n",
        "# async def main():\n",
        "#     result = await Runner.run(agent, \"Write a function that takes a list of numbers and returns the sum.\")\n",
        "#     print(result.final_output)\n",
        "\n",
        "# Execute the asynchronous main function\n",
        "# await main()\n",
        "\n",
        "\n",
        "# result = Runner.run_sync(agent, \"Write a function that takes a list of numbers and returns the sum.\")\n",
        "# print(result.final_output)\n",
        "\n",
        "\n",
        "# Option 2 : Set the model at Runner instance\n",
        "# client = AsyncOpenAI(\n",
        "#     base_url=\"http://localhost:11434/v1\"\n",
        "# )\n",
        "\n",
        "# class CustomModelProvider(ModelProvider):\n",
        "#     def get_model(self, model_name: str | None) -> Model:\n",
        "#         return OpenAIChatCompletionsModel(\n",
        "#             model = \"llama3.2\",\n",
        "#             openai_client= client\n",
        "#         )\n",
        "\n",
        "# CUSTOM_MODEL_PROVIDER = CustomModelProvider()\n",
        "\n",
        "# agent = Agent(\n",
        "#     name = \"python agent\",\n",
        "#     instructions = \"You are a helpful assistant that helps with Python programming.\",\n",
        "# )\n",
        "\n",
        "# result = Runner.run_sync(agent, \"Write a function that takes a list of numbers and returns the sum.\",\n",
        "#                          run_config = RunConfig(model_provider= CUSTOM_MODEL_PROVIDER))\n",
        "# print(result.final_output)\n",
        "\n",
        "\n",
        "# option 3 : Set the client at the global instance\n",
        "# client = AsyncOpenAI(\n",
        "#     base_url=\"http://localhost:11434/v1\"\n",
        "# )\n",
        "\n",
        "# set_default_openai_client(client)\n",
        "# set_default_openai_api(\"chat_completions\")\n",
        "\n",
        "\n",
        "# agent = Agent(\n",
        "#     name = \"python agent\",\n",
        "#     instructions = \"You are a helpful assistant that helps with Python programming.\",\n",
        "#     model = \"llama3.2\"\n",
        "# )\n",
        "\n",
        "# result = Runner.run_sync(agent, \"Write a function that takes a list of numbers and returns the sum.\")\n",
        "# print(result.final_output)\n",
        "\n",
        "\n",
        "# Option 4 : Set the model at Agent instance using Groq\n",
        "client = AsyncOpenAI(\n",
        "    api_key = userdata.get(\"GROQ_API_KEY\"),\n",
        "    base_url=\"https://api.groq.com/openai/v1\"\n",
        ")\n",
        "\n",
        "agent = Agent(\n",
        "    name = \"python agent\",\n",
        "    instructions = \"You are a helpful assistant that helps with Python programming.\",\n",
        "    model = OpenAIChatCompletionsModel(\n",
        "        model = \"llama-3.3-70b-versatile\",\n",
        "        openai_client= client\n",
        "    )\n",
        ")\n",
        "\n",
        "# run the agent\n",
        "async def main():\n",
        "    result = await Runner.run(agent, \"Write a function that takes a list of numbers and returns the sum.\")\n",
        "    print(result.final_output)\n",
        "\n",
        "# Execute the asynchronous main function\n",
        "await main()"
      ],
      "metadata": {
        "id": "WxOuxWjIUZEZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}