{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNSdJor7JEyS3RO7IWRHYDH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NormLorenz/ai-llm-openai-agents/blob/main/mcp-agent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pushover Agent\n",
        "[code](https://replit.com/@matt/OpenAI-Agents-SDK#main.py)\n",
        "[video](https://www.youtube.com/watch?v=Ta5J_2KFBGM)\n",
        "[replit](https://docs.replit.com/getting-started/intro-replit)\n",
        "[trace](https://platform.openai.com/traces)"
      ],
      "metadata": {
        "id": "qpm1QC5KK1yf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Z9FhJSNKHlNU",
        "outputId": "4aac383f-9869-4c3d-89d6-a9c6a56ed417",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting agents\n",
            "  Downloading agents-1.4.0.tar.gz (37 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.12/dist-packages (from agents) (2.19.0)\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.12/dist-packages (from agents) (0.25.2)\n",
            "Collecting ruamel.yaml (from agents)\n",
            "  Downloading ruamel_yaml-0.18.17-py3-none-any.whl.metadata (27 kB)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.12/dist-packages (from gym->agents) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from gym->agents) (3.1.2)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.12/dist-packages (from gym->agents) (0.1.0)\n",
            "Collecting ruamel.yaml.clib>=0.2.15 (from ruamel.yaml->agents)\n",
            "  Downloading ruamel_yaml_clib-0.2.15-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow->agents) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow->agents) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow->agents) (25.9.23)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow->agents) (0.7.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow->agents) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow->agents) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow->agents) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorflow->agents) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow->agents) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow->agents) (2.32.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow->agents) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow->agents) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow->agents) (3.2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow->agents) (4.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow->agents) (2.0.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow->agents) (1.76.0)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow->agents) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow->agents) (3.10.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow->agents) (3.15.1)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow->agents) (0.5.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow->agents) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow->agents) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow->agents) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow->agents) (0.18.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow->agents) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow->agents) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow->agents) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow->agents) (2025.11.12)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow->agents) (3.10)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow->agents) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow->agents) (3.1.4)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow->agents) (3.0.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow->agents) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow->agents) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow->agents) (0.1.2)\n",
            "Downloading ruamel_yaml-0.18.17-py3-none-any.whl (121 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ruamel_yaml_clib-0.2.15-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (788 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m788.2/788.2 kB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: agents\n",
            "  Building wheel for agents (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for agents: filename=agents-1.4.0-py3-none-any.whl size=62714 sha256=81efc26b33f97933f946d446061e02c8902b4f39bb9302586bf0aa1d3f2abba1\n",
            "  Stored in directory: /root/.cache/pip/wheels/e5/82/e5/2790dbbc1ad6037f1001bc436ea963e0877fff918dddc74fe2\n",
            "Successfully built agents\n",
            "Installing collected packages: ruamel.yaml.clib, ruamel.yaml, agents\n",
            "Successfully installed agents-1.4.0 ruamel.yaml-0.18.17 ruamel.yaml.clib-0.2.15\n",
            "Collecting openai-agents\n",
            "  Downloading openai_agents-0.6.4-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting griffe<2,>=1.5.6 (from openai-agents)\n",
            "  Downloading griffe-1.15.0-py3-none-any.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: mcp<2,>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from openai-agents) (1.24.0)\n",
            "Requirement already satisfied: openai<3,>=2.9.0 in /usr/local/lib/python3.12/dist-packages (from openai-agents) (2.12.0)\n",
            "Requirement already satisfied: pydantic<3,>=2.12.3 in /usr/local/lib/python3.12/dist-packages (from openai-agents) (2.12.3)\n",
            "Requirement already satisfied: requests<3,>=2.0 in /usr/local/lib/python3.12/dist-packages (from openai-agents) (2.32.4)\n",
            "Collecting types-requests<3,>=2.0 (from openai-agents)\n",
            "  Downloading types_requests-2.32.4.20250913-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.12.2 in /usr/local/lib/python3.12/dist-packages (from openai-agents) (4.15.0)\n",
            "Collecting colorama>=0.4 (from griffe<2,>=1.5.6->openai-agents)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: anyio>=4.5 in /usr/local/lib/python3.12/dist-packages (from mcp<2,>=1.11.0->openai-agents) (4.12.0)\n",
            "Requirement already satisfied: httpx-sse>=0.4 in /usr/local/lib/python3.12/dist-packages (from mcp<2,>=1.11.0->openai-agents) (0.4.3)\n",
            "Requirement already satisfied: httpx>=0.27.1 in /usr/local/lib/python3.12/dist-packages (from mcp<2,>=1.11.0->openai-agents) (0.28.1)\n",
            "Requirement already satisfied: jsonschema>=4.20.0 in /usr/local/lib/python3.12/dist-packages (from mcp<2,>=1.11.0->openai-agents) (4.25.1)\n",
            "Requirement already satisfied: pydantic-settings>=2.5.2 in /usr/local/lib/python3.12/dist-packages (from mcp<2,>=1.11.0->openai-agents) (2.12.0)\n",
            "Requirement already satisfied: pyjwt>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from pyjwt[crypto]>=2.10.1->mcp<2,>=1.11.0->openai-agents) (2.10.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.9 in /usr/local/lib/python3.12/dist-packages (from mcp<2,>=1.11.0->openai-agents) (0.0.20)\n",
            "Requirement already satisfied: sse-starlette>=1.6.1 in /usr/local/lib/python3.12/dist-packages (from mcp<2,>=1.11.0->openai-agents) (3.0.4)\n",
            "Requirement already satisfied: starlette>=0.27 in /usr/local/lib/python3.12/dist-packages (from mcp<2,>=1.11.0->openai-agents) (0.50.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.1 in /usr/local/lib/python3.12/dist-packages (from mcp<2,>=1.11.0->openai-agents) (0.4.2)\n",
            "Requirement already satisfied: uvicorn>=0.31.1 in /usr/local/lib/python3.12/dist-packages (from mcp<2,>=1.11.0->openai-agents) (0.38.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai<3,>=2.9.0->openai-agents) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from openai<3,>=2.9.0->openai-agents) (0.12.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai<3,>=2.9.0->openai-agents) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai<3,>=2.9.0->openai-agents) (4.67.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=2.12.3->openai-agents) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=2.12.3->openai-agents) (2.41.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0->openai-agents) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0->openai-agents) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0->openai-agents) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0->openai-agents) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.1->mcp<2,>=1.11.0->openai-agents) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.27.1->mcp<2,>=1.11.0->openai-agents) (0.16.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.20.0->mcp<2,>=1.11.0->openai-agents) (25.4.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.20.0->mcp<2,>=1.11.0->openai-agents) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.20.0->mcp<2,>=1.11.0->openai-agents) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.20.0->mcp<2,>=1.11.0->openai-agents) (0.30.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings>=2.5.2->mcp<2,>=1.11.0->openai-agents) (1.2.1)\n",
            "Requirement already satisfied: cryptography>=3.4.0 in /usr/local/lib/python3.12/dist-packages (from pyjwt[crypto]>=2.10.1->mcp<2,>=1.11.0->openai-agents) (43.0.3)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.12/dist-packages (from uvicorn>=0.31.1->mcp<2,>=1.11.0->openai-agents) (8.3.1)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.12/dist-packages (from cryptography>=3.4.0->pyjwt[crypto]>=2.10.1->mcp<2,>=1.11.0->openai-agents) (2.0.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12->cryptography>=3.4.0->pyjwt[crypto]>=2.10.1->mcp<2,>=1.11.0->openai-agents) (2.23)\n",
            "Downloading openai_agents-0.6.4-py3-none-any.whl (241 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.0/242.0 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading griffe-1.15.0-py3-none-any.whl (150 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.7/150.7 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading types_requests-2.32.4.20250913-py3-none-any.whl (20 kB)\n",
            "Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Installing collected packages: types-requests, colorama, griffe, openai-agents\n",
            "Successfully installed colorama-0.4.6 griffe-1.15.0 openai-agents-0.6.4 types-requests-2.32.4.20250913\n"
          ]
        }
      ],
      "source": [
        "# Install\n",
        "\n",
        "!pip install agents\n",
        "!pip install openai-agents"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "\n",
        "import os\n",
        "from datetime import datetime\n",
        "from agents import Agent, Runner, function_tool, trace\n",
        "import gradio as gr\n",
        "from google.colab import userdata\n",
        "from datetime import datetime\n",
        "import httpx\n",
        "from bs4 import BeautifulSoup\n",
        "from typing_extensions import TypedDict, Any"
      ],
      "metadata": {
        "id": "uyf88UfbMBKr"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fetch tokens for openai and pushover\n",
        "\n",
        "openai_api_key = userdata.get(\"OPENAI_API_KEY\")\n",
        "if openai_api_key:\n",
        "    print(f\"OpenAI API Key exists and starts with {openai_api_key[:8]}\")\n",
        "else:\n",
        "    print(\"OpenAI API Key not set\")\n",
        "\n",
        "pushover_user = userdata.get(\"PUSHOVER_USER\")\n",
        "if pushover_user:\n",
        "    print(f\"Pushover user found and starts with {pushover_user[0]}\")\n",
        "else:\n",
        "    print(\"Pushover user not found\")\n",
        "\n",
        "pushover_token = userdata.get(\"PUSHOVER_TOKEN\")\n",
        "if pushover_token:\n",
        "    print(f\"Pushover token found and starts with {pushover_token[0]}\")\n",
        "else:\n",
        "    print(\"Pushover token not found\")\n",
        "\n",
        "pushover_url = \"https://api.pushover.net/1/messages.json\""
      ],
      "metadata": {
        "id": "x3cPx765y5zm",
        "outputId": "c09bb286-4ae5-4203-8132-4be385116336",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OpenAI API Key exists and starts with sk-proj-\n",
            "Pushover user found and starts with u\n",
            "Pushover token found and starts with a\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The usual starting point\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = openai_api_key"
      ],
      "metadata": {
        "id": "yySK0Y57FcAT"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make an agent with name, instructions, model\n",
        "\n",
        "async def main_test():\n",
        "  agent = Agent(name=\"Jokester2\",\n",
        "            instructions=\"You are a joke teller\",\n",
        "            model=\"gpt-4o-mini\")\n",
        "\n",
        "  with trace(\"Telling a joke\"):\n",
        "    result = await Runner.run(agent, \"Tell a joke about Autonomous AI Agents\")\n",
        "    print(result.final_output)\n",
        "\n",
        "await main_test()"
      ],
      "metadata": {
        "id": "L53yxI4OGGDJ",
        "outputId": "432f11b6-7755-4647-be7a-97bf90d7120d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Why did the Autonomous AI Agent break up with its computer partner?\n",
            "\n",
            "Because it needed more space... and a better algorithm for love!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Here is the modified code to run in Colab\n",
        "\n",
        "class Location(TypedDict):\n",
        "    lat: float\n",
        "    long: float\n",
        "\n",
        "@function_tool\n",
        "async def get_time() -> str:\n",
        "    \"\"\"Fetch the current time\"\"\"\n",
        "    return datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "@function_tool\n",
        "async def fetch_url(url: str) -> str:\n",
        "    \"\"\"Fetch content from a URL\"\"\"\n",
        "    async with httpx.AsyncClient() as client:\n",
        "        response = await client.get(url)\n",
        "        return response.text\n",
        "\n",
        "@function_tool\n",
        "async def fetch_weather(location: Location) -> str:\n",
        "    \"\"\"Fetch the weather for a given location.\"\"\"\n",
        "    return \"sunny\"\n",
        "\n",
        "async def main():\n",
        "    \"\"\"Run the agent\"\"\"\n",
        "    agent = Agent(\n",
        "        name=\"Assistant\",\n",
        "        model=\"gpt-4o-mini\",\n",
        "        instructions=\"You are a helpful assistant\",\n",
        "        tools=[get_time, fetch_url]\n",
        "    )\n",
        "\n",
        "    result = await Runner.run(\n",
        "        agent, \"\"\"\n",
        "            Please get the content of https://sullivanexcavatinginc.com/ and summarize them.\n",
        "            Return the summary as well as the time you got the content.\n",
        "            \"\"\"\n",
        "    )\n",
        "    print(result.final_output)\n",
        "\n",
        "await main()"
      ],
      "metadata": {
        "id": "vDVTtv60L36l",
        "outputId": "e02cb9b0-85c2-45a0-e8c3-e84a949c2380",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I was unable to extract meaningful content from the website https://sullivanexcavatinginc.com/ as it appears to be built with dynamic content, and the page returned only its basic HTML structure without providing any visible information.\n",
            "\n",
            "The current time when I attempted to access the content is **December 29, 2025, 13:28:47**.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6b50e4a3"
      },
      "source": [
        "# Task\n",
        "Develop an agent capable of fetching content from a URL, assessing its readability, and sending a Pushover notification if the content is deemed unreadable (e.g., less than 200 characters of text after HTML parsing)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3f65451"
      },
      "source": [
        "## send_pushover_notification\n",
        "\n",
        "### Subtask:\n",
        "Define an asynchronous helper function to send messages to Pushover using the global `pushover_user`, `pushover_token`, and `pushover_url`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52fe6026"
      },
      "source": [
        "**Reasoning**:\n",
        "Define an asynchronous helper function `send_pushover_notification` to send messages to Pushover, utilizing `httpx.AsyncClient` for the POST request and global Pushover credentials.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ea02099",
        "outputId": "7832d70f-3064-4bf4-a14a-175a2fb5dc06",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "async def send_pushover_notification(message: str):\n",
        "    \"\"\"Sends a notification to Pushover.\"\"\"\n",
        "    async with httpx.AsyncClient() as client:\n",
        "        try:\n",
        "            response = await client.post(\n",
        "                pushover_url,\n",
        "                data={\n",
        "                    \"token\": pushover_token,\n",
        "                    \"user\": pushover_user,\n",
        "                    \"message\": message,\n",
        "                },\n",
        "            )\n",
        "            response.raise_for_status() # Raise an exception for HTTP errors (4xx or 5xx)\n",
        "            print(f\"Pushover notification sent successfully: {message}\")\n",
        "        except httpx.HTTPStatusError as e:\n",
        "            print(f\"Error sending Pushover notification: HTTPStatusError - {e.response.status_code} {e.response.text}\")\n",
        "        except httpx.RequestError as e:\n",
        "            print(f\"Error sending Pushover notification: RequestError - {e}\")\n",
        "\n",
        "print(\"Defined send_pushover_notification function.\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Defined send_pushover_notification function.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ba1ce292"
      },
      "source": [
        "## check_and_notify_unreadable_content\n",
        "\n",
        "### Subtask:\n",
        "Create a new `function_tool` that takes HTML content and a URL as input. It will parse the HTML using `BeautifulSoup` to extract readable text. If the extracted text length falls below a defined threshold (e.g., 200 characters), it will consider the content unreadable and call `send_pushover_notification`. It will return a string indicating whether the content was readable or not, along with a message.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6633a74"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires defining a new `function_tool` to check HTML content readability and send a Pushover notification if it's unreadable. This code block implements the `check_and_notify_unreadable_content` function with the specified logic.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cf05d7ec",
        "outputId": "9ceab6f1-7d9d-4ce9-a301-a0549085d566",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "@function_tool\n",
        "async def check_and_notify_unreadable_content(html_content: str, url: str) -> str:\n",
        "    \"\"\"Checks if HTML content is readable (based on text length) and sends a Pushover notification if not.\"\"\"\n",
        "    soup = BeautifulSoup(html_content, 'html.parser')\n",
        "    extracted_text = soup.get_text()\n",
        "\n",
        "    # Clean the extracted text\n",
        "    cleaned_text = ' '.join(extracted_text.split()).strip()\n",
        "\n",
        "    readability_threshold = 200  # Define readability threshold\n",
        "\n",
        "    if len(cleaned_text) < readability_threshold:\n",
        "        notification_message = f\"Content from {url} is unreadable (too short: {len(cleaned_text)} characters).\"\n",
        "        await send_pushover_notification(notification_message)\n",
        "        return f\"Content from {url} is unreadable (too short: {len(cleaned_text)} characters). Pushover notification sent.\"\n",
        "    else:\n",
        "        return f\"Content from {url} is readable (length: {len(cleaned_text)} characters).\"\n",
        "\n",
        "print(\"Defined check_and_notify_unreadable_content function.\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Defined check_and_notify_unreadable_content function.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81e297ce"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask has been completed by defining the `check_and_notify_unreadable_content` function. Now I will update the main agent to use the newly defined `check_and_notify_unreadable_content` tool along with `fetch_url` to achieve the overall goal of checking URL readability and sending notifications.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c926eca7",
        "outputId": "c725fd73-a380-4984-bd68-560d99792990",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "async def main_with_readability_check():\n",
        "    \"\"\"Run the agent to fetch URL content, check readability, and notify if unreadable.\"\"\"\n",
        "    agent = Agent(\n",
        "        name=\"ContentChecker\",\n",
        "        model=\"gpt-4o-mini\",\n",
        "        instructions=\"You are an agent that fetches content from a URL, checks its readability, and sends a notification if the content is unreadable (less than 200 characters of text after HTML parsing).\",\n",
        "        tools=[get_time, fetch_url, check_and_notify_unreadable_content]\n",
        "    )\n",
        "\n",
        "    print(\"\\n--- Testing with an unreadable URL ---\")\n",
        "    result_unreadable = await Runner.run(\n",
        "        agent,\n",
        "        \"Please fetch the content of https://sullivanexcavatinginc.com/, then check its readability and notify if unreadable.\"\n",
        "    )\n",
        "    print(result_unreadable.final_output)\n",
        "\n",
        "    print(\"\\n--- Testing with a readable URL ---\")\n",
        "    result_readable = await Runner.run(\n",
        "        agent,\n",
        "        \"Please fetch the content of https://www.google.com/, then check its readability and notify if unreadable.\"\n",
        "    )\n",
        "    print(result_readable.final_output)\n",
        "\n",
        "await main_with_readability_check()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Testing with an unreadable URL ---\n",
            "Pushover notification sent successfully: Content from https://sullivanexcavatinginc.com/ is unreadable (too short: 23 characters).\n",
            "The content from [Sullivan Excavating Inc](https://sullivanexcavatinginc.com/) is unreadable, containing only 23 characters. A notification has been sent regarding this issue.\n",
            "\n",
            "--- Testing with a readable URL ---\n",
            "Pushover notification sent successfully: Content from https://www.google.com/ is unreadable (too short: 6 characters).\n",
            "The content from [Google](https://www.google.com/) was found to be unreadable, containing only 6 characters. A Pushover notification has been sent regarding this issue.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5e02dcf5"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "*   An asynchronous function `send_pushover_notification` was successfully defined, capable of sending messages to Pushover using `httpx.AsyncClient`, with built-in error handling for HTTP and request-related issues.\n",
        "*   A `function_tool` named `check_and_notify_unreadable_content` was created. This tool utilizes `BeautifulSoup` to parse HTML, extract, and clean text, and then checks if the cleaned text length falls below a 200-character readability threshold. If deemed unreadable, it triggers the `send_pushover_notification` function.\n",
        "*   Testing with `https://sullivanexcavatinginc.com/` confirmed its content as unreadable, with only 23 characters of extracted text, and a Pushover notification was successfully sent.\n",
        "*   Testing with `https://www.google.com/` also resulted in the content being identified as unreadable, with only 6 characters of extracted text, leading to a Pushover notification. This demonstrates that the readability assessment strictly adheres to the raw text character count after HTML parsing.\n",
        "\n",
        "### Insights or Next Steps\n",
        "*   The current readability assessment, based solely on character count, may produce false positives for pages that are visually rich or interactive with minimal raw text content (e.g., `https://www.google.com/`).\n",
        "*   To improve accuracy, consider integrating more advanced readability metrics or filtering mechanisms that focus on visible, meaningful text content, rather than just the total extracted character count from raw HTML.\n"
      ]
    }
  ]
}
